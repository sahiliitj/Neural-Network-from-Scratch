# -*- coding: utf-8 -*-
"""Assignment 3 ML

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qVt1Q5ppGDyFZBtb0kwuEls7uu2VpiAg
"""

from google.colab import drive
drive.mount('/content/drive')

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

dataset = '/content/drive/MyDrive/Machine Learning 2023/Dataset/data.csv'

img_data = pd.read_csv(dataset)

img_data.head(10)

img_data.describe()

# In this section we are trying to find out the count of different labels.
labels = img_data['label']
uni_lab, count_lab = np.unique(labels, return_counts = True)
num_lab = {label: count for label, count in zip(uni_lab, count_lab)}
print(f"{'Label No.':<12}{'Occurences':>12}")
print('-' * 25)
for label, count in num_lab.items():
    print(f"{label:<12}{count:>12}")

# This code is used to check the occurences of different labels in training dataset
import seaborn as sns
plt.figure(figsize = (8,8))
sns.countplot(x = 'label', data = img_data, palette='dark')
plt.xlabel("Label Digit of the Image")
plt.ylabel("Count of the label digit")
plt.show()

# Pixel Intensity Distribution Plot among all the images
import matplotlib.pyplot as plt
import seaborn as sns
B = img_data.values
pixels= B.reshape(-1)
plt.figure(figsize = (10,8))
sns.histplot(pixels, kde= False,color="red")
plt.title('Pixels Intensity Distribution among images in the given dataset')
plt.xlabel('Intensity of pixels')
plt.ylabel('Count')
plt.show()

# Converting the data frames into numpy arrays for our sake of easiness
y = img_data['label'].values
X = img_data.drop('label', axis =1 ).values

# Standardising the given dataset
mean = X.mean()
std = X.std()
X = (X-mean)/std

# Shuffling the datapoints, for the training purpose
ind = np.arange(img_data.shape[0])
np.random.shuffle(ind)
print(" Here are the indices of some of the shuffled datapoints:-\n\n ", ind[: 300])
shuf_lab = y[ind]
shuf_X = X[ind]

# Splitting the whole dataset into the training and test subset of dataset
split = float(input("Enter the split ratio, (e.g if split is 70:30, give input as ~~ 0.7 ~~) :- "))
n_train = int(split*img_data.shape[0])
x_train = shuf_X[:n_train]
y_train = shuf_lab[:n_train]
x_test = shuf_X[n_train:]
y_test = shuf_lab[n_train:]
print(f" \nThe size of the training dataset is:- {x_train.shape} \nand that of testing dataset is {x_test.shape}")

# Defining the architecture of the neural network as given
layer_in= 784 #Input layer has 784 neurons
layer_hidden = [128,64,32] # We have three hidden layers with 128,64,32 neurons
layer_out = 10 #Output layer has 10 neurons as we have 10 classes

# Initialising the weights for the architecture and setting seed value as per instructions
seed_value = int(input("Enter the last 3 digits of your roll no as seed value:- "))
np.random.seed(seed_value)
w1 = np.random.randn(layer_in, layer_hidden[0])
w2 = np.random.randn(layer_hidden[0],layer_hidden[1])
w3 = np.random.randn(layer_hidden[1],layer_hidden[2])
w4 = np.random.randn(layer_hidden[2],layer_out)

# Initialising the bias = 1 for the architecture of our neural network
b = 1
b1 = np.full(layer_hidden[0], b, dtype=np.float64)
b2 = np.full(layer_hidden[1], b, dtype=np.float64)
b3 = np.full(layer_hidden[2], b, dtype=np.float64)
b4 = np.full(layer_out, b, dtype=np.float64)

# Defining the loss function for our network
def loss_func(y_actual, y_predicted):
    y_actual = np.array(y_actual)
    y_predicted = np.array(y_predicted)
    err = 1e-20
    y_predicted = np.clip(y_predicted, err, 1 - err)
    loss = -np.sum(y_actual * np.log(y_predicted)) / len(y_actual)
    return loss

# Defining the activation function, Here we chosen Sigmoid function
def sigmoid(x):
  return 1/(1+np.exp(-x))

# For applying softmax to the output layer, we are defining it here
def softmax(x):
  y = [0]*len(x)
  for i in range(0,len(x)):
    total = np.sum(np.exp(x[i]))
    x[i] = np.exp(x[i])
    y[i] = x[i]/total
  return y

# Defining the feed forward function to implement it into the network
def feed_fwd(X,w1,b1,w2,b2,w3,b3,w4,b4):
  a1= np.dot(X,w1) + b1
  h1= sigmoid(a1)
  a2= np.dot(h1,w2) + b2
  h2= sigmoid(a2)
  a3= np.dot(h2,w3) + b3
  h3= sigmoid(a3)
  a4= np.dot(h3,w4) + b4
  h4= softmax(a4)
  return h1, h2, h3, h4

# Defining the derivative of sigmoid function
def sig_der(x):
  return sigmoid(x) * (1-sigmoid(x))

# Implementing the backpropagation algorithm by defining the function below:-
def backprop(h1,h2,h3,h4,w1,w2,w3,w4,ohy):
    d4 = h4 - ohy
    d3 = sig_der(h3) * np.dot(d4,w4.T)
    d2 = sig_der(h2) * np.dot(d3,w3.T)
    d1 = sig_der(h1) * np.dot(d2,w2.T)
    return d1,d2,d3,d4

# Gradient descent algorithm for weights updation
def grad_w(etta, w1,w2,w3,w4,del_w1,del_w2,del_w3,del_w4,batch_x):
    p  = batch_x.shape[0]
    w1 -= etta * del_w1 / p
    w2 -= etta * del_w2 / p
    w3 -= etta * del_w3 / p
    w4 -= etta * del_w4 / p
    return w1,w2,w3,w4

# Gradient descent algorithm for bias values updation
def grad_b(etta,b1,b2,b3,b4,d_b1,d_b2,d_b3,d_b4,batch_x):
    t = batch_x.shape[0]
    b1 -= etta * d_b1 / t
    b2 -= etta * d_b2 / t
    b3 -= etta * d_b3 / t
    b4 -= etta * d_b4 / t
    return b1,b2,b3,b4

# Function to update the weights and bias update using gradient descent algorithm
def grad_calc(h1,h2,h3,h4,d1,d2,d3,d4,batch_x):
    d_w4 = np.dot(h3.T,d4)
    d_w3 = np.dot(h2.T,d3)
    d_w2 = np.dot(h1.T,d2)
    d_w1 = np.dot(batch_x.T,d1)
    d_b4 = np.sum(d4,axis = 0)
    d_b3 = np.sum(d3,axis = 0)
    d_b2 = np.sum(d2,axis = 0)
    d_b1 = np.sum(d1,axis = 0)
    return d_b1,d_b2,d_b3,d_b4,d_w1,d_w2,d_w3,d_w4

itr = int(input("Enter the number of epochs:- "))
b_size = int(input("Enter batch size :- "))

# Implementing the network by first feedforwarding, then using the
# gradient descent and updating the weights using backtracking
etta= 0.01  # Initialising the learning rate
num_batches = len(x_train) // b_size
acc_hist, loss_hist = [],[]

# Training the network in the given number of epochs
for i in range(itr):
  itr_loss,ccount = 0,0
    # Iterating over the batches
  for j in range(0, len(x_train), b_size):
    end = j + b_size
    if end>len(x_train):
      end = len(x_train)
    batch_x = x_train[j:end]
    batch_y = y_train[j:end]
    # Modeling the indices as one hot encoding representation by a vector
    ohy = np.eye(layer_out)[batch_y.astype(int)]
    # Feedforwarding the network
    h1,h2,h3,h4 = feed_fwd(batch_x, w1,b1,w2,b2,w3,b3,w4,b4)
    # Calculating the loss values
    loss = loss_func(h4,ohy)
    itr_loss += loss
    #Computing the accuracy of the network
    pred = np.argmax(h4,axis=1)
    ccount += np.sum(pred == batch_y)
    # Applying Backpropagation Algorithm
    d1,d2,d3,d4 = backprop(h1,h2,h3,h4,w1,w2,w3,w4,ohy)
    # Calculating the gradients
    d_b1,d_b2,d_b3,d_b4,d_w1,d_w2,d_w3,d_w4 = grad_calc(h1,h2,h3,h4,d1,d2,d3,d4,batch_x)
    # Updating the weights and bias values using the gradient descent algorithm
    w1,w2,w3,w4 = grad_w(etta, w1,w2,w3,w4,d_w1,d_w2,d_w3,d_w4,batch_x)
    b1,b2,b3,b4 = grad_b(etta,b1,b2,b3,b4,d_b1,d_b2,d_b3,d_b4,batch_x)
  itr_loss /= num_batches
  # Appending the loss after every epoch in the list
  loss_hist.append(itr_loss)
  itr_acc = ccount / len(x_train)
  # Appending the accuracy after every epoch in the list
  acc_hist.append(itr_acc)
  # Printing the Loss and Accuracy obtained after every epoch
  print(f"Epoch:{i+1}, Loss:- {itr_loss:.4f} and Accuracy:- {itr_acc*100:.2f}% ")

path_to_file ='/content/drive/MyDrive/Machine Learning 2023/Dataset/Parameters3.txt'
file = open(path_to_file,'w')
# Saving the array in a text file
file.write(f"\n \n The weight W1 matrix after training is \n :- {w2} ")
file.write(f"\n \n The weight W2 matrix after training is \n :- {w1} ")
file.write(f"\n \n The weight W3 matrix after training is \n :- {w3} ")
file.write(f"\n \n The weight W4 matrix after training is \n :- {w4} ")
file.write(f"\n \n The Bias value matrix b1 after training is \n :- {b1} ")
file.write(f"\n \n The Bias value matrix b2 after training is \n :- {b2} ")
file.write(f"\n \n The Bias value matrix b3 after training is \n :- {b3} ")
file.write(f"\n \n The Bias value matrix b4 after training is \n :- {b4} ")
file.close()

# Plotting the training loss curve
import matplotlib.pyplot as plt
plt.figure(figsize=(10,8))
plt.plot(loss_hist, label=' Loss ', color ='red')
plt.title('Loss during the training')
plt.xlabel('Number of epochs')
plt.ylabel('Values of the loss occured during the training')

# Plotting the Accuracy curve, achieved during the training
import matplotlib.pyplot as plt
plt.figure(figsize=(10,8))
plt.plot(acc_hist, label=' Loss ', color ='red')
plt.title('Achieved accuracy during the training')
plt.xlabel('Number of epochs')
plt.ylabel('Accuracy of the model during the training')

# Testing our model on a dataset
# After the training loop, evaluate the model on the test set
test_a1, test_a2, test_a3, test_a4 = feed_fwd(x_test, w1, b1, w2, b2, w3, b3, w4, b4)
test_pred = np.argmax(test_a4, axis=1)
test_acc = np.mean(test_pred == y_test)
print(f" Accuracy on the test dataset : {test_acc * 100:.5f}%")

from sklearn.metrics import confusion_matrix
# Creating a confusion matrix
cmat = confusion_matrix(y_test, test_pred)
plt.figure(figsize=(8, 6))
sns.set(font_scale=1.0)
sns.heatmap(cmat, annot=True, fmt='d', cmap='Blues', linewidths=.5, linecolor='gray', cbar=False)
plt.xlabel('Predicted Values', fontsize=12)
plt.ylabel('Actual Values (True values)', fontsize=12)
plt.title('Confusion Matrix for our trained model ', fontsize=15)
plt.xticks(fontsize=10)
plt.yticks(fontsize=10)
plt.show()